{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f532aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5f1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from databricks.connect import DatabricksSession\n",
    "\n",
    "# # Create a Spark session connected to your Databricks cluster\n",
    "# spark = DatabricksSession.builder.profile(\"dbc-df5087bc-8b50\").getOrCreate()\n",
    "\n",
    "# # Read the table from Databricks\n",
    "# df = spark.read.csv(\"dbfs:/Volumes/maven/default/data/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# # Show the first 5 rows\n",
    "# df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb8757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/data.csv\"\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5f5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe information\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590d8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"default.payment.next.month\": \"default\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5212b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names to have only the first letter capitalized\n",
    "df.columns = df.columns.str.capitalize()\n",
    "\n",
    "# Display the modified DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b8b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical values for numerical colunns\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cebf58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution\n",
    "\n",
    "# Selecting numerical features from the DataFrame\n",
    "df_num = df.select_dtypes(exclude=[\"object\"])\n",
    "\n",
    "# Loop through each numerical feature\n",
    "for numerical_feature in df_num.columns:\n",
    "    # Creating two subplots per numerical_feature\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "    # Histogram to get an overview of the distribution of each numerical_feature\n",
    "    ax[0].set_title(f\"Distribution of: {numerical_feature}\")\n",
    "    ax[0].hist(df_num[numerical_feature], bins=30, color=\"blue\", alpha=0.7, edgecolor=\"black\")\n",
    "\n",
    "    # Adding kernel density estimate (KDE)\n",
    "    kde_x = np.linspace(df_num[numerical_feature].min(), df_num[numerical_feature].max(), 100)\n",
    "    kde_y = np.exp(-0.5 * ((kde_x - df_num[numerical_feature].mean()) / df_num[numerical_feature].std()) ** 2) / (\n",
    "        df_num[numerical_feature].std() * np.sqrt(2 * np.pi)\n",
    "    )\n",
    "    ax[0].plot(kde_x, kde_y, color=\"orange\")\n",
    "\n",
    "    # Boxplot to detect outliers\n",
    "    ax[1].set_title(f\"Boxplot of: {numerical_feature}\")\n",
    "    ax[1].boxplot(df_num[numerical_feature], patch_artist=True, boxprops=dict(facecolor=\"green\", color=\"black\"))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff8b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Education.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d336e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 0, 5, and 6 with 4 in the Education column\n",
    "df[\"Education\"] = df[\"Education\"].replace({0: 4, 5: 4, 6: 4})\n",
    "\n",
    "# Display the updated value counts\n",
    "print(df[\"Education\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2114f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Marriage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dae2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 0, 5, and 6 with 4 in the Education column\n",
    "df[\"Marriage\"] = df[\"Marriage\"].replace({0: 3})\n",
    "\n",
    "# Display the updated value counts\n",
    "print(df[\"Marriage\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 0, 5, and 6 with 4 in the Education column\n",
    "df[\"Marriage\"] = df[\"Marriage\"].replace({0: 3})\n",
    "\n",
    "# Display the updated value counts\n",
    "print(df[\"Marriage\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ad2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_replace = [\"Pay_0\", \"Pay_2\", \"Pay_3\", \"Pay_4\", \"Pay_5\", \"Pay_6\"]\n",
    "df[columns_to_replace] = df[columns_to_replace].replace({-1: 0, -2: 0})\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df[\"Pay_6\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1074d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Sex\", \"Default\"]].groupby([\"Sex\"]).mean().sort_values(by=\"Default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d6938",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Default\"\n",
    "\n",
    "predictors = [\n",
    "    \"Limit_bal\",\n",
    "    \"Sex\",\n",
    "    \"Education\",\n",
    "    \"Marriage\",\n",
    "    \"Age\",\n",
    "    \"Pay_0\",\n",
    "    \"Pay_2\",\n",
    "    \"Pay_3\",\n",
    "    \"Pay_4\",\n",
    "    \"Pay_5\",\n",
    "    \"Pay_6\",\n",
    "    \"Bill_amt1\",\n",
    "    \"Bill_amt2\",\n",
    "    \"Bill_amt3\",\n",
    "    \"Bill_amt4\",\n",
    "    \"Bill_amt5\",\n",
    "    \"Bill_amt6\",\n",
    "    \"Pay_amt1\",\n",
    "    \"Pay_amt2\",\n",
    "    \"Pay_amt3\",\n",
    "    \"Pay_amt4\",\n",
    "    \"Pay_amt5\",\n",
    "    \"Pay_amt6\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Default\", \"Id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbb6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score  # Import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Sample dataset\n",
    "X = df.drop(columns=[\"Default\", \"Id\"])\n",
    "y = df[\"Default\"]\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Create LGBMClassifier\n",
    "model = LGBMClassifier(force_row_wise=True)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"boosting_type\": [\"gbdt\"],\n",
    "    \"objective\": [\"binary\"],\n",
    "    \"learning_rate\": [0.05],\n",
    "    \"scale_pos_weight\": [50],  # Adjust for class imbalance\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"roc_auc\",  # Use AUC as the scoring metric\n",
    "    cv=2,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Cross-Validation AUC Score:\", best_score)\n",
    "\n",
    "# Validate the model on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_proba = best_model.predict_proba(X_valid)[:, 1]  # Get probabilities for the positive class\n",
    "auc = roc_auc_score(y_valid, y_pred_proba)\n",
    "\n",
    "print(\"Validation AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8b077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create LGBMClassifier\n",
    "model = LGBMClassifier(force_row_wise=True)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"boosting_type\": [\"gbdt\"],\n",
    "    \"objective\": [\"binary\"],\n",
    "    \"learning_rate\": [0.05],\n",
    "    \"scale_pos_weight\": [50],  # Adjust this based on class imbalance\n",
    "}\n",
    "\n",
    "# Create a CalibratedClassifierCV with the model\n",
    "calibrated_model = CalibratedClassifierCV(estimator=model, method=\"sigmoid\")\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"roc_auc\",  # Use AUC as the scoring metric\n",
    "    cv=2,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Cross-Validation Score (AUC):\", best_score)\n",
    "\n",
    "# Validate the model on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fit the calibrated model on the training data\n",
    "calibrated_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted classes and probabilities\n",
    "y_pred = calibrated_model.predict(X_valid)\n",
    "y_pred_proba = calibrated_model.predict_proba(X_valid)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(y_valid, y_pred_proba)\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(y_valid, y_pred, output_dict=True)\n",
    "\n",
    "# Extract metrics from the classification report\n",
    "f1 = class_report[\"1\"][\"f1-score\"]\n",
    "recall = class_report[\"1\"][\"recall\"]\n",
    "precision = class_report[\"1\"][\"precision\"]\n",
    "\n",
    "# Print results\n",
    "print(\"Validation AUC:\", auc)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_valid, y_pred))\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c47196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create LGBMClassifier\n",
    "model = LGBMClassifier(force_row_wise=True, is_unbalance=True)\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"boosting_type\": [\"gbdt\"],\n",
    "    \"objective\": [\"binary\"],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    # 'scale_pos_weight': [50],  # Adjust for class imbalance\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"roc_auc\",  # Use AUC as the scoring metric\n",
    "    cv=2,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Cross-Validation AUC Score:\", best_score)\n",
    "\n",
    "# Validate the model on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_proba = best_model.predict_proba(X_valid)[:, 1]  # Get probabilities for the positive class\n",
    "auc = roc_auc_score(y_valid, y_pred_proba)\n",
    "\n",
    "print(\"Validation AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1148a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create LGBMClassifier\n",
    "model = LGBMClassifier(force_row_wise=True)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"boosting_type\": [\"gbdt\"],\n",
    "    \"objective\": [\"binary\"],\n",
    "    \"learning_rate\": [0.05],\n",
    "    \"scale_pos_weight\": [50],  # Adjust this based on class imbalance\n",
    "}\n",
    "\n",
    "# Create a CalibratedClassifierCV with the model\n",
    "calibrated_model = CalibratedClassifierCV(estimator=model, method=\"sigmoid\")\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"roc_auc\",  # Use AUC as the scoring metric\n",
    "    cv=2,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Cross-Validation Score (AUC):\", best_score)\n",
    "\n",
    "# Validate the model on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fit the calibrated model on the training data\n",
    "calibrated_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted classes and probabilities\n",
    "y_pred = calibrated_model.predict(X_valid)\n",
    "y_pred_proba = calibrated_model.predict_proba(X_valid)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(y_valid, y_pred_proba)\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(y_valid, y_pred, output_dict=True)\n",
    "\n",
    "# Extract metrics from the classification report\n",
    "f1 = class_report[\"1\"][\"f1-score\"]\n",
    "recall = class_report[\"1\"][\"recall\"]\n",
    "precision = class_report[\"1\"][\"precision\"]\n",
    "\n",
    "# Print results\n",
    "print(\"Validation AUC:\", auc)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_valid, y_pred))\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
